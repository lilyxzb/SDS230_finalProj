---
title: "Final_project_sds230"
output: word_document
date: "2024-07-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(TeachingDemos)
library(plyr)
library(dplyr)
library(tidyr)
library(TeachingDemos)
library(plyr)
#source("https://raw.githubusercontent.com/talgalili/R-code-snippets/master/boxplot.with.outlier.label.r") # boxplot with outlier function
source("/Users/lilybroach/Desktop/YaleSDS230/regJDRS.txt")
wb <- read.csv("/Users/lilybroach/Desktop/YaleSDS230/final_proj/WB.2016.csv")

```

# Introduction

## data explanation

## data cleaning


## descriptive plots, summary information



```{r}

```

# Analysis


## T Test
*We are invigating whether there is a significant difference in mean  forest area in 1994 and 2014 using paired t test, asking the question is the overall change positive or negative*
```{r}
summary(wb$Forest94)
summary(wb$Forest14)
wb$diffForest <- wb$Forest94 - wb$Forest14
t.test(wb$diffForest)
```


## Correlation 
*discuss assumptinos of correlation and linear regressin, which i believe is a GLM so need to talk ab..out that too* 
Linear regression is a type of generalized linear model used to .. the assumptions of GLM is that we have random, normally distributed errors centered at zero with constant variance. There should be homoskedasticity, with constant variance across groups. In addition, for simple linear regression, the variables must be linearly related.
*Introduction to quesiton we are investigation. try logit to measles immunization, may need to subtract when taking logit, correlated to infant mortality?*
First, we want to investigate whether there is a relationship between infant mortality and measles vaccination rates. We check the distribution of the variables and see that Measles vaccination rates are heavily left skewed and infant mortality is heavily right skewed. We will likely need to perform some transformations ot these variables so that they are more symmetrical and uniformly distributed.
```{r}
# remove NAs 
mort <- na.omit(wb %>% select(Measles,InfMort))
hist(mort$Measles)
hist(mort$InfMort)
```
### Boxcox Transformation 

```{r}
# determine best transformation for lm1, 
lm1 <- lm(InfMort ~ Measles, data = mort)

trans1 <- boxCox(lm1)
trans1$x[which.max(trans1$y)]
```
*applying power of .22 transformation, plot 1, both, the other, compare the distributions between all of them*
```{r}
# both raised to .22
mort$transMeasles <- (mort$Measles)**.22
mort$transInfMort <- (mort$InfMort)**.22

lmb <- lm(transInfMort ~ transMeasles, data = mort)
summary(lmb)
myResPlots2(lmb)
mtext("Infant Mortality^.22 and Measles^.22")

#infmort raised to .22
lmi <- lm(transInfMort ~ Measles, data = mort)
summary(lmi)
myResPlots2(lmi)
mtext("Infant Mortality^.22")

# measles rasied to .22 
lmm <-  lm(InfMort ~ transMeasles, data = mort)
summary(lmm)
myResPlots2(lmm)
mtext("Measles^.22")

```
Even after raising the variables in various combinations to the power of .22, we see non-normality and severe heteroskedasticity in the residual plots for all the combinations of raising the variables to lambda. The variables may not be linearlly related, and this violates the assumption of linearity for correlation testing.

*should i plot each of these transformation combinations before bootstrapping for confidence interval and slope?*
*may delete the following chunk, if not*
```{r, echo=FALSE}
# correlation test
cor1 <- cor(mort$Measles,mort$InfMort)

# fit linear regression model
lm1 <- lm(InfMort ~ Measles, data = mort)

#plot 
plot(InfMort ~ Measles, data = mort, main = "Infant Mortality Rate vs Measles Immunization Rate", xlab = "% of Measles Immunization for 12-23 month olds", ylab = "Infant Mortality Rate per 1,000 Live Births",
     pch = 19, col = "red")
mtext(paste("r = ", round(cor1, 2) , ", slope = ", round(lm1$coef[2], 2)))
abline(lm1$coef, col = "blue", lwd = 3)

```


*therefore, we do boostrap confidence interval for correlation and slope to try and get an estimate of the true correlation, since the assumptions for linear regression adn correlation ahvebeen violated (firstly because residuals show hetereoskedasticity and secondly the residuals don't follow the normal distribution as much as we may hope), we need to do non parametric testing ie confidence intervals*

### Bootstrap CI for Correlation and Slope
```{r}
N <- nrow(mort)

#Specify how many boostrap samples to take
n_samp <- 10000

corResults <- rep(NA, n_samp)
bResults <- rep(NA, n_samp) 

for(i in 1:n_samp){
  #get vector of rows in our fake sample
  s <- sample(1:N, N , replace = T)
  fakeData <-  mort[s, ]
    
  #Get bootstrapped correlation and regression slope
  corResults[i] <- cor(fakeData[, 1], fakeData[, 2])
  bResults[i] <- lm(fakeData[, 2] ~ fakeData[, 1])$coef[2]

}

#Get percentiles for 2.5 and 97.5
ci_r <- quantile(corResults, c(.025, .975))
ci_slope <- quantile(bResults, c(.025, .975))

#Histogram of bootstrapped correlation values with CI's (both bootstrapped and theoretical)
hist(corResults, col = "blue", main = "Bootstrapped Correlations", xlab = "Sample Correlation", breaks = 50)
abline(v = ci_r, lwd = 3, col = "red")
abline(v = cor.test(mort$Measles, mort$InfMort)$conf.int, lwd = 3, col = "green", lty = 2)
legend(-.4, 350, c("Theoretical CI","Boot CI"), lwd = 3, col = c("green","red"), lty = c(2, 1))


#get regression results again
lm1 <- lm(InfMort ~ Measles, data = mort)

#Histogram of bootstrapped regression slopes with CI's (both bootstrapped and theoretical)
hist(bResults, col = "blue", main = "Bootstrapped Slopes", xlab = "Sample Slope", breaks = 50)
abline(v = ci_slope, lwd = 3, col = "red")
abline(v = confint(lm1,'Measles'), lwd = 3, col = "green", lty = 2)
legend("topleft", c("Theoretical CI","Boot CI"), lwd = 3, col = c("green","red"), lty = c(2, 1))

#reminder of regression results
summary(lm1)

qqPlot(corResults, main = "Normal Quantile Plot of Bootstrapped Correlation")
qqPlot(bResults, main = "Normal Quantile Plot of Bootstrapped Slopes")
```

*what do the bootstrapped CI for corrrelation and slope suggest? go to mod 11. BOotstrap CI captures more variability of the correlation and slopes, perhaps due to the violated assumptions of correlatin in that there is some heteroskedasticity that we saw earlier in the fit vs residuals. Although we did bootstrap the data to find bootstrapped confidence intervals for correlation adn slope, since we are not confident the data itself is linearlly associated, the effectiveness of doing a correlation test is perhaps not very strong *
The right skew in the bootstrapped correlation is the histogram is captured well in the  normal quantile plot. The right tail is a bit longer than the left, causing a deviation from normality. We can see that the bootstrapped correlation confidence intervals captures more variability inherent to the data to due to the right skew. The bootstrapped slopes show a slight left skew which also indicates deviations from normality. In both cases of correlation and slope, there are variabilities in the data not captured by the parametric theoretical tests, which could be a reason why the bootstrapped confidence intervals are wider than the theoretical intervals. The non-parametric bootstrapping in both cases is a better alternative to the theoretical correlation and slope because it casts a wider net to better capture the true characteristics of the data without making assumptions of normality and homeskedasticity.




### Correlation between % of total Population in rural areas vs GNI per capita 
Another set of variables that may be linearly related are the % of the population in rural areas and GNI. First, we take a look at the distribution of the variables to see if the assumptions of correlation and linear regression are met.
```{r}
summary(wb$Rural) #has 0s and NA
summary(wb$GNI) # has NA
ruralGNI <- na.omit(wb %>% select(Rural,GNI,Country))


#take a look at % rural area variable
hist(ruralGNI$Rural, main = "Histogram of % Population in Rural Area", xlab = "%", col = "orange", breaks = 30) 
qqPlot(ruralGNI$Rural, main = "Normal Quantile plot of % Population in Rural Area")
```





*Looking at the histogram of % rural population is not particularly informative by itself, but looking at the normal quantile plot tells us that the data is close to but not exactly normally distributed. On both tails of the distribution, there are deviations from the noraml distribution and its a bit wider than we would like. Since we are looking at percentage, we can try a logit transformation that transforms the data using log(%/(100-%)). *

```{r}
summary(ruralGNI$Rural)# has zeros, so need to add some amount to every observavtion
sort(ruralGNI$Rural)
logit(ruralGNI$Rural + .02)
#hist(logit(ruralGNI$Rural + .02))
#hist(logit(ruralGNI$Rural + .2))
#hist(logit(ruralGNI$Rural + .5))

ruralGNI$logitRural <- logit(ruralGNI$Rural + .2)
summary(ruralGNI$logitRural)

hist(ruralGNI$logitRural, main = "Histogram of logit % Population in Rural Area", col = "orange") 
qqPlot(ruralGNI$logitRural, main = "Histogram of logit % Population in Rural Area")
```
Performing a logit transformation on the % population in rural areas made the histogram more symetrical, although there is still a left skew. The Normal quantile plot looks better, but we can still see there are a few countries that have an unexpectedly low amount of % living in rural areas. 
```{r}
summary(ruralGNI$logitRural)
sort(ruralGNI$logitRural) # what are the countries with less than -6 logit rural?
(urban <- ruralGNI[ruralGNI$logitRural < -6,])

```
The countries with extremely low % percentage of population living in rural areas include Singapore, Hong Kong, and Singapore, all of  which are city states. These cities have high urban area with relatively higher population density compared to other countries. Nauru is a beach island in Oceania. Nauru has an extremely low rural population. *should i link wikipedia site where i found the rural population for nauru is 0??*

Next, we can look at the distribution of GNI per capita.


```{r}
# take a look at GNI
hist(ruralGNI$GNI, main = "Histogram of GNI per capita", col = "green") # extremely right skewed. try log transformation
qqPlot(ruralGNI$GNI, main = "Histogram of GNI per capita")
```
Since the distribution looks very right skewed and GNI is very large numbers in the thousands, we may try a log transformation, which does look better. The data approximates a normal distribution and is more unfirom and symmetrical, although it shoul dbe noted there are a few more countries with higher GNI than we may expect for a normal distirbution, and these obsevations do stick out in the histogram and normal quantile plot.

```{r}
# try log transformation
summary(ruralGNI$GNI) # no zeros present
ruralGNI$logGNI <- log(ruralGNI$GNI)
hist(ruralGNI$logGNI, main = "Histogram of log GNI per capita", col = "green") # more symmetrical 
qqPlot(ruralGNI$logGNI, main = "Histogram of log GNI per capita")

```
We can take our transformed variables, log GNI per capita and logit % of population in rural areas and fit them to a linear model and determine their correlation coefficients, slope, and rsquared. Pearson's r has a value of -.66 indicating there is a semi-strong negative correlation between our variables of interest. By performing a t-test with 185 degrees of freedom, we get a p value of 2E-16 which is well below the alpha level .05. Therefore we can reject the null hypothesis that the true correlation is not equal to zero. Additionally, with a statistically significant log GNI coefficient of -.68 , we can see that for every one unit increase in log GNI, we can expect a .68 decrease in logit % of people living in a rural area. The rsquared reveals that about 44% of the variance of the response variable can be explained by the independent variable. Overall, we can conclude that performing simple linear regression and correlation is a good fit for the data, and logit rural population % and log GNI are negatively correlated. This is not a surprise, since higher GNI could be thought of as a  proxy for technological development which post industrial revolution, which is typically associated with urbanization. 
```{r}
#fit linear regression model
lm3 <- lm(logitRural ~ logGNI, data = ruralGNI)
# is the model significant
summary(lm3)
# calculate correlation 
cor3 <- cor(ruralGNI$logGNI,ruralGNI$logitRural)
# Is the correlation significant?
cor.test(ruralGNI$logGNI,ruralGNI$logitRural)
#plot % rural vs log GNI 
plot(logitRural ~ logGNI, data = ruralGNI, main = "logit Rural Population (% of Total Population) vs log GNI", xlab = "log GNI per capita", ylab = "logit Rural Population %",
     pch = 19, col = "violet")
mtext(paste("r = ", round(cor3, 2) , ", rsquared = ", round((abs(cor3))**2,2) , ", slope = ", round(lm3$coef[2], 2)))
abline(lm3$coefficients, col = "blue", lwd = 3)

```
The outliers we see with high GNI per capita and low % rural are the same outliers we saw before, plus Qatar. These countries all have higher GNI and lower rural areas. Their influence is something to keep in mind when interpreting the correlation and slope of this linear regression.
```{r}
ruralGNI[ruralGNI$logitRural < -4 & ruralGNI$logGNI > 9,]
ruraledited <- ruralGNI[ruralGNI$logitRural < -4 & ruralGNI$logGNI > 9,]
```



In addition, we can see if the model residuals are normally and uniformally distributed
```{r}
myResPlots2(lm3)

```
*approximates normality, some outliers but all in all looks like good application for linear regression adn that gni per capita prdicts % populaiton in rural area pretty well*


### Multiple Regression
#### look at response variable Military Expenditures
```{r}
summary(wb$Military) #note: there are NA and 0 in data
boxplot(wb$Military, col = "red", main = "Military Expenditures (% of GDP)", horizontal = T, xlab = "% military expenditure")
hist(wb$Military, col = "red", main = "Military Expenditures (% of GDP)", xlab = "% military expenditure")
qqPlot(wb$Military, main = "Military Expenditures (% of GDP)", pch = 19)
```
*The data is heavily right skewed and not normally distributed. These plots all suggest we might want to look at things on the logit because its %. logit function is log(p/(1-p)) or it is log(%/(100-%))*

```{r}


wb$logitMilitary <- logit(wb$Military + .2) # since there are zeros in the data
sort(wb$logitMilitary)
summary(wb$logitMilitary)
hist(wb$logitMilitary)
qqPlot(wb$logitMilitary, pch = 19)
boxplot(wb$logitMilitary, col = "red", main = "Logit Military Expenditures % of GDP", ylab = "logit military expenditure")


```
*Now the data seems more reasonably normally distributed and we begin to look at the predictor variables. We notice that there are a few potential outliers spending more and less than what we might expect from a normal distribution. We can see the countries that these outliers belong to below.*

```{r, fig.height=6}
boxplot.with.outlier.label(wb$logitMilitary, wb$Country, col = "red", ylab = "logit military expenditure")
```


*Now that our response variable is transformed, we can begin to look at the relationships with this transformed variable and some potential explanatory variables. First, we make correlation plot of all the possible predictors we want to include in our model*
```{r}

```







## conclusion and summary
