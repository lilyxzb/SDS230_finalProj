---
title: "Final_project_sds230"
output: word_document
date: "2024-07-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(TeachingDemos)
library(plyr)
library(dplyr)
library(tidyr)
library(TeachingDemos)
library(plyr)
source("https://raw.githubusercontent.com/talgalili/R-code-snippets/master/boxplot.with.outlier.label.r") # boxplot with outlier function
source("/Users/lilybroach/Desktop/YaleSDS230/regJDRS.txt")
wb <- read.csv("/Users/lilybroach/Desktop/YaleSDS230/final_proj/WB.2016.csv")

```

# Introduction

## data explanation

## data cleaning


## descriptive plots, summary information



```{r}

```

# Analysis


## T Test



## Correlation 
*discuss assumptinos of correlation and linear regressin, which i believe is a GLM so need to talk ab..out that too* 
Linear regression is a type of generalized linear model used to .. the assumptions of GLM is that we have random, normally distributed errors centered at zero with constant variance. There should be homoskedasticity, with constant variance across groups. In addition, for simple linear regression, the variables must be linearly related.
*Introduction to quesiton we are investigation. try logit to measles immunization, may need to subtract when taking logit, correlated to infant mortality?*
First, we want to investigate whether there is a relationship between infant mortality and measles vaccination rates. We check the distribution of the variables and see that Measles vaccination rates are heavily left skewed and infant mortality is heavily right skewed. We fit an inital linear model to these variables and calculate correlation and rsquared value.
```{r}
# remove NAs 
mort <- na.omit(wb %>% select(Measles,InfMort))
hist(mort$Measles)
hist(mort$InfMort)

# fit linear regression model
lm1 <- lm(InfMort ~ Measles, data = mort)

# correlation test
cor1 <- cor(mort$Measles,mort$InfMort)


#plot 
plot(InfMort ~ Measles, data = mort, main = "Infant Mortality Rate vs Measles Immunization Rate", xlab = "% of Measles Immunization for 12-23 month olds", ylab = "Infant Mortality Rate per 1,000 Live Births",
     pch = 19, col = "red")
mtext(paste("r = ", round(cor1, 2) , ", rsquared = ", round(cor1^2,2),", slope = ", round(lm1$coef[2], 2)))
abline(lm1$coef, col = "blue", lwd = 3)
#look at residuals 

summary(mort$Measles)
```
There is a moderate - strong negative relationship between % Measles immunizations and infant mortality, and the rsquared of .31 represents that 31% of the variability in infant mortality rate can be explained by this model. However, since measles immunization is piled towards 100%, this casues probelms of spread and variance. We can look at the residual plots to investiagte this issue further.

```{r}
myResPlots2(lm1)
```

There looks to be some hetereoskedasticity in the fit vs studentized residuals, likely due to the extreme right skew of measles vacciination. This makes sense, as the median measles % vaccination is 93 and the mean is 87.21; most of the data is centered on the right.  Since measles vaccination is a percentage, we can perform logit transformation and see if this improves the fit. 
```{r}
mort$logitMeasles <- logit(mort$Measles)
hist(mort$logitMeasles)
summary(mort$logitMeasles)
```
After taking logit of % Measles vaccination, we can see in the histogram is is more evenly distributed. There is still a left skew, but this is to be expected since countries still have high measles vaccination rates. With our transformed predictor, we make another scatterplot and fit a new regression model.


```{r}
# fit linear regression model
lm2 <- lm(InfMort ~ logitMeasles, data = mort)

# correlation test
cor2 <- cor(mort$logitMeasles,mort$InfMort)


#plot 
plot(InfMort ~ logitMeasles, data = mort, main = "Infant Mortality Rate vs logit Measles Immunization Rate", xlab = "logit % of Measles Immunization for 12-23 month olds", ylab = "Infant Mortality Rate per 1,000 Live Births",
     pch = 19, col = "red")
mtext(paste("r = ", round(cor2, 2) ,", rsquared = ", round(cor2^2,2), ", slope = ", round(lm1$coef[2], 2)))
abline(lm2$coef, col = "blue", lwd = 3)


myResPlots2(lm2)


```
*In the new scatterplot with logit measles, the data is more dispersed along the x axis. The spread and linear assumptions of correlation and linear models are better met with the transformed variable. The measles histogram is less skewed, the residual plot has less unequal variance, and the overall model fit is better with the transformed predictor variable.  With an rsquared value of .28, 28% of the variability in infant mortality can be explained by this model. While this is lower than the squared value of the previous model (rsquared = .31), the current model with logit measles vaccination is a better fit since the underlying assumptions are better met. The rsquared value is only one way of interpreting good model fit, and we don't always necessarily want the model with the highest rsquared.*


*do bootstrapping as comparison to parametric function*
### Bootstrap CI for Correlation and Slope

```{r}
mort2 <- mort %>% dplyr::select(logitMeasles,InfMort)
N <- nrow(mort2)

#Specify how many boostrap samples to take
n_samp <- 10000

corResults <- rep(NA, n_samp)
bResults <- rep(NA, n_samp) 

for(i in 1:n_samp){
  #get vector of rows in our fake sample
  s <- sample(1:N, N , replace = T)
  fakeData <-  mort2[s, ]
    
  #Get bootstrapped correlation and regression slope
  corResults[i] <- cor(fakeData[, 1], fakeData[, 2])
  bResults[i] <- lm(fakeData[, 2] ~ fakeData[, 1])$coef[2]

}

#Get percentiles for 2.5 and 97.5
ci_r <- quantile(corResults, c(.025, .975))
ci_slope <- quantile(bResults, c(.025, .975))

#Histogram of bootstrapped correlation values with CI's (both bootstrapped and theoretical)
hist(corResults, col = "blue", main = "Bootstrapped Correlations", xlab = "Sample Correlation", breaks = 50)
abline(v = ci_r, lwd = 3, col = "red")
abline(v = cor.test(mort2$logitMeasles, mort2$InfMort)$conf.int, lwd = 3, col = "green", lty = 2)
legend(-.4, 350, c("Theoretical CI","Boot CI"), lwd = 3, col = c("green","red"), lty = c(2, 1))


#get regression results again
lm2 <- lm(InfMort ~ logitMeasles, data = mort)

#Histogram of bootstrapped regression slopes with CI's (both bootstrapped and theoretical)
hist(bResults, col = "blue", main = "Bootstrapped Slopes", xlab = "Sample Slope", breaks = 50)
abline(v = ci_slope, lwd = 3, col = "red")
abline(v = confint(lm2,'logitMeasles'), lwd = 3, col = "green", lty = 2)
legend("topleft", c("Theoretical CI","Boot CI"), lwd = 3, col = c("green","red"), lty = c(2, 1))

#reminder of regression results
summary(lm1)


```
The histograms above show the bootstrapped data both correlation and slope, the bootstrapped confidence intervals are a bit wider than the theoretical. In both cases, the bootstrapped confidence intervals are more conservative than the theoretical, which makes sense because the bootstrapping is non parametric therefore capturing more of the true variability in the data. We can look at normal quantile plots of the bootstrapped data to further visualize the distribution.

```{r}
qqPlot(corResults, main = "Normal Quantile Plot of Bootstrapped Correlation")
qqPlot(bResults, main = "Normal Quantile Plot of Bootstrapped Slopes")
```
As expected, distributions for correlation and slope approximate reflect normality. There is a slight right skew in the normal quantile plot of bootstrapped correlation which is also reflected in the histogram.  The histogram for slope looks very near normal, and the data falls almost entirely along the straight line in the normal quantile plot. 


## Multiple regression


### Multiple Regression
#### look at response variable Military Expenditures
```{r}
summary(wb$Military) #note: there are NA and 0 in data
boxplot(wb$Military, col = "red", main = "Military Expenditures (% of GDP)", horizontal = T, xlab = "% military expenditure")
hist(wb$Military, col = "red", main = "Military Expenditures (% of GDP)", xlab = "% military expenditure")
qqPlot(wb$Military, main = "Military Expenditures (% of GDP)", pch = 19)
```
*The data is heavily right skewed and not normally distributed. These plots all suggest we might want to look at things on the logit because its %. logit function is log(p/(1-p)) or it is log(%/(100-%))*

```{r}


wb$logitMilitary <- logit(wb$Military + .2) # since there are zeros in the data
sort(wb$logitMilitary)
summary(wb$logitMilitary)
hist(wb$logitMilitary)
qqPlot(wb$logitMilitary, pch = 19)
boxplot(wb$logitMilitary, col = "red", main = "Logit Military Expenditures % of GDP", ylab = "logit military expenditure")


```
*Now the data seems more reasonably normally distributed and we begin to look at the predictor variables. We notice that there are a few potential outliers spending more and less than what we might expect from a normal distribution. We can see the countries that these outliers belong to below.*

```{r, fig.height=6}
boxplot.with.outlier.label(wb$logitMilitary, wb$Country, col = "red", ylab = "logit military expenditure",ylim = c(-7,-1))
ylim = c(-7,-1)
```
*pointing to same points that are zero*

*Now that our response variable is transformed, we can begin to look at the relationships with this transformed variable and some potential explanatory variables. First, we make correlation plot of all the possible predictors we want to include in our model*
```{r}

```







## conclusion and summary
